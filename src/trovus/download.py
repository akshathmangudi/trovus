#!/usr/bin/env python3
"""
Trovus Download Module

Handles downloading model weights from HuggingFace Hub with flexible options.
Supports selective downloads, model card integration, and efficient caching.

Authors: Trovus Research Team
"""

import os
import sys
import argparse
from pathlib import Path
from typing import List, Optional, Dict, Any
import re

from huggingface_hub import snapshot_download, hf_hub_download, HfApi, hf_hub_url
from huggingface_hub.errors import RepositoryNotFoundError, RevisionNotFoundError
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.table import Table
from rich.prompt import Confirm
from rich import print as rprint

console = Console()


class ModelDownloader:
    """
    Handles downloading models from HuggingFace Hub with various options.
    """
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        Initialize the model downloader.
        
        Args:
            cache_dir: Custom cache directory. If None, uses HF default.
        """
        self.api = HfApi()
        self.cache_dir = cache_dir
        self.console = Console()
    
    def parse_model_identifier(self, identifier: str) -> Dict[str, Any]:
        """
        Parse a model identifier to extract repo_id and other metadata.
        
        Args:
            identifier: Model identifier (e.g., "microsoft/DialoGPT-medium", "gpt2")
            
        Returns:
            Dict with parsed information
        """
        # Handle shorthand models (expand to full repo paths)
        shorthand_mapping = {
            "gpt2": "openai-community/gpt2",
            "distilbert": "distilbert-base-uncased",
            "bert": "google-bert/bert-base-uncased",
        }
        
        if identifier in shorthand_mapping:
            identifier = shorthand_mapping[identifier]
        
        # Validate format
        if "/" not in identifier and identifier not in shorthand_mapping.values():
            # Assume it's a model name, try to find it
            console.print(f"[yellow]Warning: '{identifier}' doesn't follow 'owner/model' format. Searching...[/yellow]")
            return {"repo_id": identifier, "needs_search": True}
        
        return {"repo_id": identifier, "needs_search": False}
    
    def get_model_info(self, repo_id: str) -> Optional[Dict[str, Any]]:
        """
        Get model information from HuggingFace Hub.
        
        Args:
            repo_id: Repository ID
            
        Returns:
            Model information dictionary or None if error
        """
        try:
            model_info = self.api.model_info(repo_id)
            
            # Calculate approximate size from files
            total_size = 0
            file_info = []
            
            if hasattr(model_info, 'siblings') and model_info.siblings:
                for file in model_info.siblings:
                    if hasattr(file, 'size') and file.size:
                        total_size += file.size
                        file_info.append({
                            "filename": file.rfilename,
                            "size": file.size
                        })
            
            return {
                "repo_id": repo_id,
                "total_size": total_size,
                "files": file_info,
                "tags": getattr(model_info, 'tags', []),
                "library_name": getattr(model_info, 'library_name', None),
                "model_info": model_info
            }
            
        except RepositoryNotFoundError:
            console.print(f"[red]Error: Model '{repo_id}' not found on HuggingFace Hub.[/red]")
            return None
        except Exception as e:
            console.print(f"[red]Error fetching model info: {e}[/red]")
            return None
    
    def format_size(self, size_bytes: int) -> str:
        """Format file size in human readable format."""
        size = float(size_bytes)
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f}{unit}"
            size /= 1024.0
        return f"{size:.1f}TB"
    
    def filter_files(self, files: List[Dict], include_patterns: Optional[List[str]] = None, 
                    exclude_patterns: Optional[List[str]] = None, specific_files: Optional[List[str]] = None) -> List[str]:
        """
        Filter files based on patterns and specific file requests.
        
        Args:
            files: List of file dictionaries
            include_patterns: Patterns to include (e.g., ["*.safetensors"])
            exclude_patterns: Patterns to exclude (e.g., ["*.bin"])
            specific_files: Specific files to download
            
        Returns:
            List of filtered filenames
        """
        import fnmatch
        
        if specific_files:
            return specific_files
        
        filtered_files = []
        
        for file_info in files:
            filename = file_info["filename"]
            
            # Apply include patterns
            if include_patterns:
                if not any(fnmatch.fnmatch(filename, pattern) for pattern in include_patterns):
                    continue
            
            # Apply exclude patterns
            if exclude_patterns:
                if any(fnmatch.fnmatch(filename, pattern) for pattern in exclude_patterns):
                    continue
            
            filtered_files.append(filename)
        
        return filtered_files
    
    def download_model(self, repo_id: str, 
                      output_dir: Optional[str] = None,
                      include_patterns: Optional[List[str]] = None,
                      exclude_patterns: Optional[List[str]] = None,
                      specific_files: Optional[List[str]] = None,
                      revision: str = "main",
                      force_download: bool = False,
                      resume: bool = True) -> Optional[str]:
        """
        Download a model from HuggingFace Hub.
        
        Args:
            repo_id: Repository ID to download
            output_dir: Local directory to download to
            include_patterns: File patterns to include
            exclude_patterns: File patterns to exclude
            specific_files: Specific files to download
            revision: Git revision to download
            force_download: Force re-download even if cached
            resume: Resume interrupted downloads
            
        Returns:
            Path to downloaded model directory or None if error
        """
        # Get model info first
        model_info = self.get_model_info(repo_id)
        if not model_info:
            return None
        
        # Show download info
        console.print(f"\n[bold green]Downloading {repo_id}[/bold green]")
        console.print(f"Total size: {self.format_size(model_info['total_size'])}")
        
        # Filter files if needed
        if include_patterns or exclude_patterns or specific_files:
            filtered_files = self.filter_files(
                model_info['files'], 
                include_patterns, 
                exclude_patterns, 
                specific_files
            )
            
            # Calculate filtered size
            filtered_size = sum(
                f["size"] for f in model_info['files'] 
                if f["filename"] in filtered_files
            )
            
            console.print(f"Filtered size: {self.format_size(filtered_size)}")
            console.print(f"Files to download: {len(filtered_files)}")
        
        # Confirm download if large
        if model_info['total_size'] > 1_000_000_000:  # 1GB
            if not Confirm.ask(f"Model is {self.format_size(model_info['total_size'])}. Continue download?"):
                console.print("[yellow]Download cancelled.[/yellow]")
                return None
        
        try:
            # Download with progress
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeRemainingColumn(),
                console=console
            ) as progress:
                
                task = progress.add_task(f"Downloading {repo_id}...", total=None)
                
                # Use snapshot_download for full downloads or filtered downloads
                if specific_files and len(specific_files) == 1:
                    # Single file download
                    downloaded_path = hf_hub_download(
                        repo_id=repo_id,
                        filename=specific_files[0],
                        revision=revision,
                        cache_dir=self.cache_dir,
                        local_dir=output_dir,
                        force_download=force_download,
                        resume_download=resume
                    )
                else:
                    # Multiple files or full repo download
                    downloaded_path = snapshot_download(
                        repo_id=repo_id,
                        revision=revision,
                        cache_dir=self.cache_dir,
                        local_dir=output_dir,
                        allow_patterns=include_patterns,
                        ignore_patterns=exclude_patterns,
                        force_download=force_download,
                        resume_download=resume
                    )
                
                progress.update(task, completed=True)
            
            console.print(f"[bold green]âœ“ Successfully downloaded to: {downloaded_path}[/bold green]")
            return downloaded_path
            
        except Exception as e:
            console.print(f"[red]Error downloading model: {e}[/red]")
            return None


def download_command(args):
    """Handle the download subcommand."""
    downloader = ModelDownloader(cache_dir=args.cache_dir)
    
    # Parse model identifier
    model_info = downloader.parse_model_identifier(args.model)
    
    if model_info["needs_search"]:
        console.print(f"[yellow]Model '{args.model}' needs disambiguation. Please use full 'owner/model' format.[/yellow]")
        return
    
    repo_id = model_info["repo_id"]
    
    # Prepare download options
    include_patterns = args.include if args.include else None
    exclude_patterns = args.exclude if args.exclude else None
    specific_files = args.files if args.files else None
    
    # Handle preset modes
    if args.minimal:
        # Include all common model weight formats + essential configs
        include_patterns = [
            "config.json", "tokenizer.json", "tokenizer_config.json",
            "*.safetensors", "*.bin", "*.h5", "*.model"
        ]
        exclude_patterns = ["*.msgpack", "*.onnx", "*.tflite"]  # Exclude less common formats
        console.print("[blue]Using minimal download mode (configs + model weights)[/blue]")
    elif args.research_mode:
        # Include all model weights but exclude very large or unnecessary files
        include_patterns = [
            "*.safetensors", "*.bin", "*.h5", "*.model", "*.json", 
            "*.txt", "*.py", "*.md"  # Include configs, tokenizers, and docs
        ]
        exclude_patterns = [
            "*.msgpack", "*.onnx", "*.tflite", "*.gguf",  # Exclude specialized formats
            "*tf_model*", "*flax_model*"  # Exclude framework-specific duplicates when possible
        ]
        console.print("[blue]Using research mode (all weights + configs, exclude specialized formats)[/blue]")
    
    # Download the model
    downloaded_path = downloader.download_model(
        repo_id=repo_id,
        output_dir=args.output_dir,
        include_patterns=include_patterns,
        exclude_patterns=exclude_patterns,
        specific_files=specific_files,
        revision=args.revision,
        force_download=args.force,
        resume=not args.no_resume
    )
    
    if downloaded_path:
        console.print(f"\n[bold]Model downloaded successfully![/bold]")
        console.print(f"Location: {downloaded_path}")


def download_from_card_command(args):
    """Handle downloading from a model card."""
    console.print("[yellow]Model card download functionality coming soon![/yellow]")
    # TODO: Implement model card parsing and download


def add_download_subparser(subparsers):
    """Add download-related subcommands to the argument parser."""
    
    # Main download command
    download_parser = subparsers.add_parser(
        'download', 
        help='Download model weights from HuggingFace Hub',
        description='Download models with flexible filtering options'
    )
    
    download_parser.add_argument(
        'model',
        help='Model identifier (e.g., microsoft/DialoGPT-medium, Qwen/Qwen3-0.6B)'
    )
    
    download_parser.add_argument(
        '--output-dir', '-o',
        help='Local directory to download model to'
    )
    
    download_parser.add_argument(
        '--cache-dir',
        help='Custom cache directory (default: HuggingFace cache)'
    )
    
    download_parser.add_argument(
        '--include',
        nargs='+',
        help='File patterns to include (e.g., *.safetensors *.json)'
    )
    
    download_parser.add_argument(
        '--exclude',
        nargs='+',
        help='File patterns to exclude (e.g., *.bin *.msgpack)'
    )
    
    download_parser.add_argument(
        '--files',
        nargs='+',
        help='Specific files to download'
    )
    
    download_parser.add_argument(
        '--revision',
        default='main',
        help='Git revision to download (default: main)'
    )
    
    download_parser.add_argument(
        '--minimal',
        action='store_true',
        help='Download only essential files (configs + model weights: safetensors/bin/h5/model)'
    )
    
    download_parser.add_argument(
        '--research-mode',
        action='store_true',
        help='Download optimized for research (all weights + configs, exclude specialized formats)'
    )
    
    download_parser.add_argument(
        '--force',
        action='store_true',
        help='Force re-download even if cached'
    )
    
    download_parser.add_argument(
        '--no-resume',
        action='store_true',
        help='Disable resuming interrupted downloads'
    )
    
    download_parser.set_defaults(func=download_command)
    
    # Download from card command
    card_parser = subparsers.add_parser(
        'download-card',
        help='Download model from a model card file',
        description='Download models using information from saved model cards'
    )
    
    card_parser.add_argument(
        'card_path',
        help='Path to the model card file'
    )
    
    card_parser.add_argument(
        '--output-dir', '-o',
        help='Local directory to download model to'
    )
    
    card_parser.set_defaults(func=download_from_card_command)
